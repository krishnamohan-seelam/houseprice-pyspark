{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bbde224-c2be-4823-a0c1-2980a06fe9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Housing price prediction\n",
    "# Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to a north-south railroad. House price negotiations often have a lot of influencing factors and not just the number of bedrooms or the position of the kitchen.\n",
    "# \n",
    "# Take the given dataset with 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. In this hackathon, predict the final price of each home. \n",
    "# \n",
    "# The application should be modeled using Machine Learning, you may explore libraries such as PySpark. Apply containerization principles as a better software engineering practice. You may explore Kafka server for streaming the data.\n",
    "# \n",
    "# The model can be deployed using Docker containers for scalability.\n",
    "# \n",
    "# Dataset: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/\n",
    "# \n",
    "# Keywords: ML at scale, feature engineering, regression, random forest, gradient boosting, Distributed ML, Spark, Kafka, Containers\n",
    "\n",
    "# from IPython.core.display import HTML\n",
    "# display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer,OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import RobustScaler\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.regression import LinearRegression,LinearRegressionModel\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VarianceThresholdSelector\n",
    "from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "\n",
    "### all the CONSTANTS used in the program\n",
    "\n",
    "TRAIN_FILE = \"house-prices/train.csv\"\n",
    "SDV_TRAIN_FILE = \"house-prices/train_sdv.csv\"\n",
    "TEST_FILE = \"house-prices/test.csv\"\n",
    "PREDICTION_OUTPUT_LOCATION_PROD = \"/localuser/\"\n",
    "PREDICTION_OUTPUT_LOCATION_DEV = \"/home/jovyan/work/\"\n",
    "TEST_STRUCT_FIELDS =  [\n",
    " StructField('Id', IntegerType(), True),\n",
    " StructField('MSSubClass', IntegerType(), True),\n",
    " StructField('MSZoning', StringType(), True),\n",
    " StructField('LotFrontage', IntegerType(), True),\n",
    " StructField('LotArea', IntegerType(), True),\n",
    " StructField('Street', StringType(), True),\n",
    " StructField('Alley', StringType(), True),\n",
    " StructField('LotShape', StringType(), True),\n",
    " StructField('LandContour', StringType(), True),\n",
    " StructField('Utilities', StringType(), True),\n",
    " StructField('LotConfig', StringType(), True),\n",
    " StructField('LandSlope', StringType(), True),\n",
    " StructField('Neighborhood', StringType(), True),\n",
    " StructField('Condition1', StringType(), True),\n",
    " StructField('Condition2', StringType(), True),\n",
    " StructField('BldgType', StringType(), True),\n",
    " StructField('HouseStyle', StringType(), True),\n",
    " StructField('OverallQual', IntegerType(), True),\n",
    " StructField('OverallCond', IntegerType(), True),\n",
    " StructField('YearBuilt', IntegerType(), True),\n",
    " StructField('YearRemodAdd', IntegerType(), True),\n",
    " StructField('RoofStyle', StringType(), True),\n",
    " StructField('RoofMatl', StringType(), True),\n",
    " StructField('Exterior1st', StringType(), True),\n",
    " StructField('Exterior2nd', StringType(), True),\n",
    " StructField('MasVnrType', StringType(), True),\n",
    " StructField('MasVnrArea', IntegerType(), True),\n",
    " StructField('ExterQual', StringType(), True),\n",
    " StructField('ExterCond', StringType(), True),\n",
    " StructField('Foundation', StringType(), True),\n",
    " StructField('BsmtQual', StringType(), True),\n",
    " StructField('BsmtCond', StringType(), True),\n",
    " StructField('BsmtExposure', StringType(), True),\n",
    " StructField('BsmtFinType1', StringType(), True),\n",
    " StructField('BsmtFinSF1', IntegerType(), True),\n",
    " StructField('BsmtFinType2', StringType(), True),\n",
    " StructField('BsmtFinSF2', IntegerType(), True),\n",
    " StructField('BsmtUnfSF', IntegerType(), True),\n",
    " StructField('TotalBsmtSF', IntegerType(), True),\n",
    " StructField('Heating', StringType(), True),\n",
    " StructField('HeatingQC', StringType(), True),\n",
    " StructField('CentralAir', StringType(), True),\n",
    " StructField('Electrical', StringType(), True),\n",
    " StructField('1stFlrSF', IntegerType(), True),\n",
    " StructField('2ndFlrSF', IntegerType(), True),\n",
    " StructField('LowQualFinSF', IntegerType(), True),\n",
    " StructField('GrLivArea', IntegerType(), True),\n",
    " StructField('BsmtFullBath', IntegerType(), True),\n",
    " StructField('BsmtHalfBath', IntegerType(), True),\n",
    " StructField('FullBath', IntegerType(), True),\n",
    " StructField('HalfBath', IntegerType(), True),\n",
    " StructField('BedroomAbvGr', IntegerType(), True),\n",
    " StructField('KitchenAbvGr', IntegerType(), True),\n",
    " StructField('KitchenQual', StringType(), True),\n",
    " StructField('TotRmsAbvGrd', IntegerType(), True),\n",
    " StructField('Functional', StringType(), True),\n",
    " StructField('Fireplaces', IntegerType(), True),\n",
    " StructField('FireplaceQu', StringType(), True),\n",
    " StructField('GarageType', StringType(), True),\n",
    " StructField('GarageYrBlt', IntegerType(), True),\n",
    " StructField('GarageFinish', StringType(), True),\n",
    " StructField('GarageCars', IntegerType(), True),\n",
    " StructField('GarageArea', IntegerType(), True),\n",
    " StructField('GarageQual', StringType(), True),\n",
    " StructField('GarageCond', StringType(), True),\n",
    " StructField('PavedDrive', StringType(), True),\n",
    " StructField('WoodDeckSF', IntegerType(), True),\n",
    " StructField('OpenPorchSF', IntegerType(), True),\n",
    " StructField('EnclosedPorch', IntegerType(), True),\n",
    " StructField('3SsnPorch', IntegerType(), True),\n",
    " StructField('ScreenPorch', IntegerType(), True),\n",
    " StructField('PoolArea', IntegerType(), True),\n",
    " StructField('PoolQC', StringType(), True),\n",
    " StructField('Fence', StringType(), True),\n",
    " StructField('MiscFeature', StringType(), True),\n",
    " StructField('MiscVal', IntegerType(), True),\n",
    " StructField('MoSold', IntegerType(), True),\n",
    " StructField('YrSold', IntegerType(), True),\n",
    " StructField('SaleType', StringType(), True),\n",
    " StructField('SaleCondition', StringType(), True),\n",
    "]\n",
    "\n",
    "\n",
    "COLS_TO_UPPER = ['ExterQual', 'ExterCond', 'KitchenQual', 'FireplaceQu', 'GarageFinish',\n",
    "                 'GarageQual', 'GarageCond', 'PoolQC', 'BsmtQual', 'BsmtCond',\n",
    "                 'HeatingQC', 'BsmtFinType1', 'BsmtFinType2','BsmtExposure','MSSubClass','SaleCondition','Fence']\n",
    "\n",
    "COLS_TO_INT = ['BsmtFinType1', 'BsmtFinType2','ExterQual', 'ExterCond', 'KitchenQual', 'FireplaceQu','GarageQual', 'PoolQC',\n",
    "               'BsmtQual','BsmtCond','HeatingQC','GarageCond','GarageFinish','BsmtExposure','MSSubClass','SaleCondition','Fence']\n",
    "\n",
    "QUALITY_COLS =['ExterQual', 'ExterCond', 'KitchenQual', 'FireplaceQu','GarageQual', 'PoolQC','BsmtQual','BsmtCond','HeatingQC','GarageCond']\n",
    "\n",
    "ORDINAL_CATEGORY_FEATURES = ['GarageYrBlt','MoSold','YrSold']\n",
    "\n",
    "EXCLUDE_COLS  = ['SalePrice','LogSalePrice','Id']\n",
    "BASEMENT_FINISH_CONVERTER ={ 'NO BASEMENT':'1', 'NA': '1', 'UNF':'2', 'LWQ':'3','REC':'4','BLQ':'5','ALQ':'6','GLQ':'7'}\n",
    "GARAGE_FINISH_MAP={'NA': '1', \"UNF\": '2', \"RFN\": '3', \"FIN\": '4'} \n",
    "QUALITY_CONVERTER = {'NA':'1','NO FIREPLACE':'1','NO POOL':'1','NO BASEMENT':'1','NO GARAGE':'1',\n",
    "                     'PO':'2', 'FA':'3','TA':'4' ,'GD':'5','EX' :'6'}\n",
    "\n",
    "BSMTEXPOSURE_MAP = { 'NA':'1' ,'NO':'2','MN':'3','AV':'4','GD':'5'}\n",
    "MSSUBCLASS_MAP = {'30':'1','180':'2','45':'2','190':'2','90':'2','150':'2','160':'2',\n",
    "                  '50':'3','85':'3','40':'3','70':'4','80':'4','20':'4','75':'4','120':'4','60':'5'}\n",
    "SALECONDITION_MAP ={'ABNORML': '2', 'ALLOCA': '2', 'ADJLAND': '2', 'FAMILY': '2', 'NORMAL': '1', 'PARTIAL': '1'}\n",
    "FENCE_MAP ={'NA': '1', \"MNWW\": '2', \"GDWO\": '3', \"MNPRV\": '4', \"GDPRV\": '5'} \n",
    "\n",
    "TRAIN_SCHEMA = StructType(TEST_STRUCT_FIELDS + [StructField('SalePrice', IntegerType(), True)])\n",
    "TEST_SCHEMA = StructType(TEST_STRUCT_FIELDS)\n",
    "\n",
    "IMPUTE_COLS =[ 'LotFrontage', 'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF',\n",
    "                        'BsmtFullBath','BsmtHalfBath','GarageYrBlt','GarageCars','GarageArea']\n",
    "\n",
    "def df_shape(spark_df):\n",
    "    return spark_df.count(),len(spark_df.columns)\n",
    "\n",
    "def get_dataframe(spark_session,filename,schema):\n",
    "    \"\"\"Load file as spark data frame \"\"\"\n",
    "    df = spark_session.read.csv(filename,schema = schema, header= True)\n",
    "    return df\n",
    "\n",
    "def combine_dfs(all_dfs):\n",
    "    df= reduce(DataFrame.unionAll, all_dfs)\n",
    "    return df\n",
    "\n",
    "def get_null_counts(py_df):\n",
    "    \"\"\"Computes the value count of each column and reports null value count \"\"\"\n",
    "    results = {}\n",
    "    for cname,ctype in py_df.dtypes:      \n",
    "        results[cname] = py_df.where( py_df[cname].isNull() ).count()    \n",
    "    null_counts= {k:v for k,v in results.items() if v!=0 }  \n",
    "    return null_counts\n",
    "\n",
    "\n",
    "def impute_nulls(df):\n",
    "    \"\"\"function to Impute nulls and returns dataframe\"\"\"\n",
    "    df = df.na.fill(value=0,subset=IMPUTE_COLS)\n",
    "    return df \n",
    "\n",
    "def add_new_features(df):\n",
    "    \"\"\"function to add new features and returns dataframe\"\"\"\n",
    "    df = df.withColumn(\"TotalBath\", df[\"FullBath\"] + 0.5*df[\"HalfBath\"] + df[\"BsmtFullBath\"] + 0.5*df[\"BsmtHalfBath\"])\n",
    "    df = df.withColumn(\"TotalArea\", df[\"GrLivArea\"] + df[\"TotalBsmtSF\"])\n",
    "    df = df.withColumn(\"TotalFloorSF\",df['1stFlrSF'] + df['2ndFlrSF'])\n",
    "    df = df.withColumn(\"RemodelledAge\",  df['YearRemodAdd']-df['YearBuilt'])\n",
    "    df = df.withColumn(\"Age\",  2010 -df['YearBuilt'])\n",
    "    df = df.withColumn(\"IsRegularLotShape\" , F.when(df.LotShape ==  \"Reg\",\"1\").otherwise(\"0\")) \n",
    "    df = df.withColumn(\"IsRemodeled\" , F.when(df.YearBuilt !=  df.YearRemodAdd,\"1\").otherwise(\"0\")) \n",
    "    df = df.withColumn(\"VeryNewHouse\" , F.when(df.YearBuilt ==  df.YrSold,\"1\").otherwise(\"0\")) \n",
    "    df = df.withColumn(\"TotalPorchSF\", df[\"OpenPorchSF\"] + df[\"EnclosedPorch\"]+df[\"3SsnPorch\"] + df[\"ScreenPorch\"])\n",
    "    df = df.drop(\"FullBath\",\"HalfBath\",\"BsmtFullBath\",\"BsmtHalfBath\",\"1stFlrSF\",\"2ndFlrSF\",\"GrLivArea\",\"TotalBsmtSF\")\n",
    "    df = df.drop('OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','YearRemodAdd','YearBuilt')\n",
    "    return df\n",
    "\n",
    "def convert_features(df):\n",
    "    \"\"\"function to convert features and returns dataframe\"\"\"\n",
    "    for col_name in COLS_TO_UPPER:\n",
    "        df =  df.withColumn(col_name,F.upper(F.col(col_name)))\n",
    "        \n",
    "    df = df.replace(BASEMENT_FINISH_CONVERTER,subset=['BsmtFinType1', 'BsmtFinType2'])\n",
    "    df = df.replace(QUALITY_CONVERTER,subset=QUALITY_COLS)\n",
    "    df = df.replace(GARAGE_FINISH_MAP,subset=['GarageFinish'])\n",
    "    df = df.replace(BSMTEXPOSURE_MAP,subset=['BsmtExposure'])\n",
    "    df = df.replace(MSSUBCLASS_MAP,subset=['MSSubClass'])\n",
    "    df = df.replace(SALECONDITION_MAP,subset=['SaleCondition'])\n",
    "    df = df.replace(FENCE_MAP,subset=['Fence'])\n",
    "    \n",
    "    for col in COLS_TO_INT:\n",
    "        df = df.withColumn(col,F.col(col).cast('integer'))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_log_sale_price(df):\n",
    "    \"\"\"function to add column LogSalePrice and returns dataframe\"\"\"\n",
    "    df = df.withColumn(\"LogSalePrice\", F.log10(F.col(\"SalePrice\")))\n",
    "    return df\n",
    "\n",
    "    \n",
    "def get_cat_and_cont_features(df,exclude_cols=None):\n",
    "    \"\"\"function to return categorical and continous features\"\"\"\n",
    "    category_features = [f.name for f in df.schema.fields if isinstance(f.dataType, T.StringType)]\n",
    "    ignore_cols  = category_features + exclude_cols if exclude_cols else category_features\n",
    "    category_features = category_features + ORDINAL_CATEGORY_FEATURES\n",
    "    continuous_features = [ f.name for f in df.schema.fields if f.name not in ignore_cols]\n",
    "    return category_features,continuous_features\n",
    "    \n",
    "\n",
    "def create_pipeline(category_features,continuous_features,add_vector_index=False):\n",
    "    \"\"\"function to create pipeline based on categorical and continous features and returns pipeline\"\"\"\n",
    "    str_enc = [ f\"{col}_str_enc\" for col in category_features]\n",
    "    ohe_enc = [ f\"{col}_ohe\" for col in category_features]\n",
    "    stage_str_enc = [StringIndexer(inputCol=a, outputCol= b,handleInvalid=\"keep\") for a,b in zip(category_features,str_enc)]\n",
    "    stage_ohe_enc = [OneHotEncoder(inputCol= a, outputCol= b) for a,b in zip(str_enc,ohe_enc) ]\n",
    "    next_input_col = 'indexed_features' if add_vector_index else 'features'\n",
    "    \n",
    "    if add_vector_index:\n",
    "        stage_vec_assembler = VectorAssembler(inputCols= continuous_features + str_enc ,outputCol=\"features\")\n",
    "        stage_indexer =    VectorIndexer(inputCol= \"features\", outputCol= \"indexed_features\",handleInvalid=\"keep\")\n",
    "    else:\n",
    "        stage_vec_assembler = VectorAssembler(inputCols= continuous_features + ohe_enc ,outputCol=\"features\")\n",
    "    \n",
    "        \n",
    "    stage_scaler = RobustScaler(inputCol=next_input_col,outputCol=\"scaled_features\")\n",
    "    stage_selector = UnivariateFeatureSelector(featuresCol='scaled_features',\n",
    "                                           outputCol=\"selected_features\",\n",
    "                                           labelCol=\"SalePrice\",\n",
    "                                           )\n",
    "\n",
    "    stage_selector.setFeatureType(\"continuous\").setLabelType(\"continuous\") \n",
    "    if add_vector_index:\n",
    "        pipeline_stages  = stage_str_enc + [stage_vec_assembler,stage_indexer,stage_scaler]\n",
    "    else:\n",
    "        pipeline_stages  = stage_str_enc + stage_ohe_enc + [stage_vec_assembler,stage_scaler]\n",
    "        \n",
    "    pipeline = Pipeline(stages= pipeline_stages)\n",
    "    return pipeline\n",
    "\n",
    "def test_model(env=\"PROD\"):\n",
    "    \"\"\"function to test the model save the results to csv \"\"\"\n",
    "    OUTPUT_LOCATION = PREDICTION_OUTPUT_LOCATION_DEV if env ==\"DEV\" else PREDICTION_OUTPUT_LOCATION_PROD\n",
    "    PREDICTION_FILE = f\"{OUTPUT_LOCATION}\"+\"predictions.csv\"\n",
    "    print(\"predictions will be stored at:\",PREDICTION_FILE)\n",
    "    spark_session = SparkSession.builder.appName(\"HousePrices\").getOrCreate()\n",
    "    print('Starting model testing')\n",
    "    df = get_dataframe(spark_session,TEST_FILE,TEST_SCHEMA)\n",
    "    print('loaded data into spark df')\n",
    "    df = impute_nulls(df)\n",
    "    print('finished imputing nulls ')\n",
    "    df = add_new_features(df)\n",
    "    print('finished adding new features')\n",
    "    df = convert_features(df)\n",
    "    print('finished adding converting features')\n",
    " \n",
    "    category_features,continuous_features = get_cat_and_cont_features(df,EXCLUDE_COLS)\n",
    "    print('finished getting category and continous features')\n",
    "    pipeline_model = PipelineModel.load('pipeline_model.h5')\n",
    "    print('finished loading pipeline')\n",
    "    lr_model = LinearRegressionModel.load('lr_model.h5')\n",
    "    test_pdf = pipeline_model.transform(df)\n",
    "    test_prediction= lr_model.transform(test_pdf)\n",
    "    test_prediction = test_prediction.select(\"Id\",\"prediction\")\n",
    "    test_prediction = test_prediction.withColumnRenamed(\"prediction\",\"SalePrice\")\n",
    "    print(test_prediction.show(5))\n",
    "    #test_prediction.repartition(1).write.format('csv').mode(\"overwrite\").options(sep=',', header='true').save(PREDICTION_FILE)\n",
    "    pred_df = test_prediction.toPandas()\n",
    "    pred_df.to_csv(PREDICTION_FILE,index=False)\n",
    "    print('Finished model testing')\n",
    "    \n",
    "#master(\"local[*]\").config('job.local.dir', 'file:/home/joyvan/work')\n",
    "def train_model(synthetic_data=False):\n",
    "    \"\"\"function to test the model save the results to csv \"\"\"\n",
    "    spark_session = SparkSession.builder.appName(\"HousePrices\").getOrCreate()\n",
    "    print('Starting model training')\n",
    "    df = get_dataframe(spark_session,TRAIN_FILE,TRAIN_SCHEMA)\n",
    "    print('loaded training data into spark df')\n",
    "    \n",
    "    if synthetic_data:\n",
    "        print(\"Shape of training data before adding synthentic data:\", df_shape(df))\n",
    "        sdf = get_dataframe(spark_session,SDV_TRAIN_FILE,TRAIN_SCHEMA)\n",
    "        print('loaded synthetic training data into spark df')\n",
    "        df = combine_dfs([df,sdf])\n",
    "        print('combined data into spark df')\n",
    "        print(\"Shape of training data after adding synthentic data:\", df_shape(df))\n",
    "        \n",
    "    df = impute_nulls(df)\n",
    "    print('finished imputing nulls ')\n",
    "    df = add_new_features(df)\n",
    "    print('finished adding new features')\n",
    "    df = convert_features(df)\n",
    "    print('finished adding converting features')\n",
    "    category_features,continuous_features = get_cat_and_cont_features(df,EXCLUDE_COLS)\n",
    "    print('finished getting category and continous features')\n",
    "    null_counts = get_null_counts(df)\n",
    "    if null_counts:\n",
    "        print(null_counts)\n",
    "        raise ValueError(\"Dataset has nulls, cannot create model\")\n",
    "   \n",
    "\n",
    "    pipeline  = create_pipeline(category_features,continuous_features)\n",
    "    print('finished creating pipeline')\n",
    "    \n",
    "   \n",
    "    train_df, test_df =  df.randomSplit([0.8,0.2], seed = 42)\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    train_pdf = pipeline_model.transform(train_df)\n",
    "    print('finished pipeline transforming training data')\n",
    "    lr =  LinearRegression(featuresCol=\"scaled_features\", labelCol=\"SalePrice\",\n",
    "                           maxIter= 10,regParam=0.3, elasticNetParam=0.8)\n",
    "    \n",
    "    print(f\"Observations in training set = {train_df.count()}\")\n",
    "    print(f\"Observations in testing set = { test_df.count()}\")\n",
    "    lr_model = lr.fit(train_pdf)\n",
    "    print('finished training model')\n",
    "    trainingSummary = lr_model.summary\n",
    "    print(trainingSummary.residuals.show()) \n",
    "    print(f\"RMSE: {trainingSummary.rootMeanSquaredError:.4f}\")\n",
    "    print(f\"r2: {trainingSummary.r2:.4f}\")\n",
    "    test_pdf = pipeline_model.transform(test_df)\n",
    "    test_prediction= lr_model.transform(test_pdf)\n",
    "    eval_lr = RegressionEvaluator(predictionCol='prediction', labelCol='SalePrice')\n",
    "    rmse= eval_lr.evaluate(test_prediction, {eval_lr.metricName:'rmse'})\n",
    "    r2 =eval_lr.evaluate(test_prediction,{eval_lr.metricName:'r2'})\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"r2: {r2:.4f}\")\n",
    "    print('Finished model creation')\n",
    "    return pipeline_model,lr_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1575eaa5-f8dc-4dbf-82ca-661f148fe96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training\n",
      "loaded training data into spark df\n",
      "Shape of training data before adding synthentic data: (1460, 81)\n",
      "loaded synthetic training data into spark df\n",
      "combined data into spark df\n",
      "Shape of training data after adding synthentic data: (2960, 81)\n",
      "finished imputing nulls \n",
      "finished adding new features\n",
      "finished adding converting features\n",
      "finished getting category and continous features\n",
      "finished creating pipeline\n",
      "finished pipeline transforming training data\n",
      "Observations in training set = 2409\n",
      "Observations in testing set = 551\n",
      "finished training model\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-11318.689025462314|\n",
      "| -21778.12092361244|\n",
      "|-44169.567879930255|\n",
      "| -48326.35379899491|\n",
      "| -20793.47739591723|\n",
      "|-49552.900250051694|\n",
      "|-2546.0027776873903|\n",
      "| 20273.086599604518|\n",
      "| -10885.97697505547|\n",
      "|  29237.97573555488|\n",
      "|  1943.424801178975|\n",
      "| -6269.324297341809|\n",
      "|-2043.3879643575929|\n",
      "| -9348.081232982106|\n",
      "|  4515.866488885455|\n",
      "| 12987.342477173137|\n",
      "|  -8157.88566465379|\n",
      "|-18302.821970648627|\n",
      "| -5777.921744744439|\n",
      "| -5476.799302500935|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "RMSE: 35133.3837\n",
      "r2: 0.8126\n",
      "RMSE: 32916.8181\n",
      "r2: 0.8303\n",
      "Finished model creation\n"
     ]
    }
   ],
   "source": [
    "pipeline_model,lr_model = train_model(synthetic_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8a63c8-4e2e-4e8b-bf0a-6f0585747a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save('pipeline_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc0b835-1ebd-4c7a-a19c-02a58691d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.write().overwrite().save('lr_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c097fd64-e8d7-4d92-8476-63fe03ad8906",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions will be stored at: /home/jovyan/work/predictions.csv\n",
      "Starting model testing\n",
      "loaded data into spark df\n",
      "finished imputing nulls \n",
      "finished adding new features\n",
      "finished adding converting features\n",
      "finished getting category and continous features\n",
      "finished loading pipeline\n",
      "+----+------------------+\n",
      "|  Id|         SalePrice|\n",
      "+----+------------------+\n",
      "|1461|114818.84553679085|\n",
      "|1462|135793.10252914464|\n",
      "|1463|178397.26143742522|\n",
      "|1464|198685.28842861866|\n",
      "|1465| 204516.0175038802|\n",
      "+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Finished model testing\n"
     ]
    }
   ],
   "source": [
    "test_model(env=\"DEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d31be-b19d-4dc9-b051-f63f1c783aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
